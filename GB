import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Datos experimentales (los mismos que en las consultas anteriores)
data = {
    "T": [15]*13 + [25]*17 + [35]*13 + [45]*13,
    "Co": [5.218, 10.569, 19.965, 30.214, 48.076, 69.958, 97.345, 126.135, 149.765, 173.83, 198.5, 228.84, 261.38,
           5.197, 10.472, 15.485, 20.238, 25.358, 29.715, 35.689, 40.782, 48.48, 69.03, 100.225, 129.12, 147.685, 176.465, 206.09, 226.19, 247.87,
           5.329, 10.756, 20.943, 30.424, 49.365, 69.03, 100.225, 129.12, 143.875, 178.405, 186.08, 214.34, 240.33,
           5.377, 10.002, 20.986, 30.867, 48.887, 69.406, 107.695, 122.385, 149.345, 177.335, 199.1, 235.74, 251.24],
    "Cte": [0.704, 1.674, 2.233, 2.530, 4.718, 12.078, 19.353, 24.474, 35.764, 56.010, 85.980, 113.885, 146.840,
            0.850, 1.172, 1.633, 2.090, 2.589, 2.832, 3.460, 4.158, 5.100, 7.733, 12.208, 19.335, 26.263, 39.264, 62.362, 78.694, 103.985,
            1.15, 1.775, 2.660, 4.061, 7.088, 9.011, 16.174, 23.143, 31.187, 47.152, 59.346, 77.748, 107.692,
            1.291, 1.882, 2.787, 3.003, 5.692, 8.384, 18.473, 21.955, 31.734, 47.326, 69.698, 100.825, 118.215],
    "Qte": [2.015, 3.971, 7.916, 9.360, 14.360, 25.839, 36.820, 43.380, 48.360, 50.120, 50.232, 50.320, 51.134,
            1.941, 4.152, 6.184, 8.102, 10.165, 12.001, 14.388, 16.350, 19.366, 27.365, 39.293, 49.904, 54.653, 62.032, 65.164, 66.846, 66.234,
            1.687, 3.563, 8.162, 12.770, 21.870, 26.794, 38.416, 47.758, 53.539, 58.700, 60.363, 61.979, 61.213,
            1.601, 3.804, 8.125, 9.140, 16.840, 24.240, 40.278, 44.835, 51.844, 57.040, 58.769, 59.230, 59.386]
}
df = pd.DataFrame(data)

# Características y variable objetivo
features = ["T", "Co", "Cte"]
target = "Qte"

# Escalado de los datos
scaler = MinMaxScaler()
X = scaler.fit_transform(df[features])
y = df[target].values

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Implementación de Gradient Boosting Regressor ---

# Inicialización del modelo con hiperparámetros
# Puedes ajustar estos parámetros para optimizar el rendimiento
gbr_params = {
    'n_estimators': 500,       # Número de árboles
    'max_depth': 4,            # Profundidad máxima de cada árbol
    'min_samples_split': 5,    # Mínimo de muestras para dividir un nodo
    'learning_rate': 0.01,     # Tasa de aprendizaje
    'loss': 'squared_error'    # Función de pérdida
}
gbr = GradientBoostingRegressor(**gbr_params)

# Entrenamiento del modelo
gbr.fit(X_train, y_train)

# Predicción sobre los datos de prueba
y_pred = gbr.predict(X_test)

# Evaluación del modelo
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f'RMSE: {rmse}')
print(f'MSE: {mse}')

# --- Análisis de Importancia de Características ---
feature_importance = gbr.feature_importances_
sorted_idx = np.argsort(feature_importance)

print("\nImportancia de las características:")
for i, feature in enumerate(np.array(features)[sorted_idx]):
    print(f"{feature}: {feature_importance[sorted_idx][i]:.4f}")

# --- Visualización de los Resultados ---
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='green', alpha=0.6, label="Valores Reales vs. Predichos")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label="Línea de Referencia (y=x)")
plt.xlabel("Valores Reales (y_test)")
plt.ylabel("Valores Predichos (y_pred)")
plt.title("Rendimiento del Modelo Gradient Boosting")
plt.legend()
plt.grid(True)
plt.show()
